{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a24c886-c1ee-4324-89f2-ec1171602cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c76a88-e4b5-4371-a4eb-129dbadb105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0b06b0-3417-4134-a2a4-9bd1c521f115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['images', 'annotations'])\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/h36m'\n",
    "subject = '1'\n",
    "action = '2'\n",
    "subaction = '1'\n",
    "with open(os.path.join(DATA_DIR, 'annotations', 'Human36M_subject1_data.json'), 'r') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca388750-9cc2-4843-ac82-d687a8ff2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautiful_json(data):\n",
    "    return json.dumps(data, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9022ec5-0256-422a-96b9-5a27f08ed6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"action_idx\": 2,\n",
      "    \"action_name\": \"Directions\",\n",
      "    \"cam_idx\": 1,\n",
      "    \"file_name\": \"s_01_act_02_subact_01_ca_01/s_01_act_02_subact_01_ca_01_000001.jpg\",\n",
      "    \"frame_idx\": 0,\n",
      "    \"height\": 1002,\n",
      "    \"id\": 0,\n",
      "    \"subaction_idx\": 1,\n",
      "    \"subject\": 1,\n",
      "    \"width\": 1000\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(beautiful_json(data['images'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386dacce-dcb1-4557-8237-3a355eda2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len([x for x in data['images'] if x[\"action_idx\"] == 2 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e8b116-470c-4f68-9caa-0a7eff36087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SMPL model\n",
    "with open(os.path.join(DATA_DIR, 'smpl_param', 'Human36M_subject{}_smpl_param.json'.format(subject)), 'r') as jsonfile:\n",
    "    smpl_data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc410bec-57de-4fb0-bf16-1f67b3d8cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Frames:  277\n"
     ]
    }
   ],
   "source": [
    "print(\"# Frames: \", len(smpl_data[action][subaction].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1f215e-e395-4c47-b56a-07eb72c69aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = []\n",
    "betas = []\n",
    "trans = []\n",
    "for frame in range(0, len(smpl_data[action][subaction].keys())):\n",
    "    key = frame * 5\n",
    "    if str(key) not in smpl_data[action][subaction].keys():\n",
    "        continue\n",
    "    pose = smpl_data[action][subaction][str(key)]['pose']\n",
    "    poses.append(pose)\n",
    "    shape = smpl_data[action][subaction][str(key)]['shape']\n",
    "    betas.append(shape)\n",
    "#     tran = smpl_data[action][subaction][str(key)]['trans']\n",
    "    trans.append([0, 0, 0]) # for ease of vis\n",
    "poses = torch.Tensor(poses).to(comp_device)\n",
    "betas = torch.Tensor(betas).to(comp_device)\n",
    "trans = torch.Tensor(trans).to(comp_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b617042-8bb6-49fa-af78-27fc757ae282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "bm_fname = '../body_models/smplh/neutral/model.npz'\n",
    "dmpl_fname = '../body_models/dmpls/neutral/model.npz'\n",
    "\n",
    "num_betas = 10 # number of body parameters\n",
    "num_dmpls = 8 # number of DMPL parameters\n",
    "\n",
    "bm = BodyModel(bm_fname=bm_fname, num_betas=num_betas, num_dmpls=num_dmpls, dmpl_fname=dmpl_fname).to(comp_device)\n",
    "faces = c2c(bm.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5ff1dd1-2cb4-4ce8-b927-5dee83e88e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from body_visualizer.tools.vis_tools import render_smpl_params\n",
    "images = render_smpl_params(bm,\n",
    "                            {'pose_body': poses[:, 3:66], \n",
    "                            'trans': trans,\n",
    "                            'betas': betas})\n",
    "images = np.expand_dims(images, axis=(0, 1))\n",
    "\n",
    "from body_visualizer.tools.vis_tools import imagearray2file\n",
    "video = imagearray2file(images, \"videos/sample-s{}-a{}-sa{}.mp4\".format(subject, action, subaction), fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee46a7d-ea44-4576-9099-326383d8b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from body_visualizer.tools.vis_tools import colors\n",
    "from body_visualizer.mesh.mesh_viewer import MeshViewer\n",
    "from body_visualizer.mesh.sphere import points_to_spheres\n",
    "from body_visualizer.tools.vis_tools import show_image\n",
    "\n",
    "imw, imh = 1000, 1000\n",
    "mv = MeshViewer(width=imw, height=imh, use_offscreen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37ad8ab4-b9c4-4d57-8ee2-68368bc824ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_pose_beta = bm(**{k:v for k,v in body_parms.items() if k in ['pose_body', 'betas']})\n",
    "\n",
    "body_pose_beta = bm(pose_body=poses[:, 3:66], \n",
    "                    betas=betas)\n",
    "def vis_body_pose_beta(fId = 0):\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body_pose_beta.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "# vis_body_pose_beta(fId=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd992f2-7163-4738-a1c0-41480933e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pose', 'shape', 'trans', 'fitted_3d_pose'])\n"
     ]
    }
   ],
   "source": [
    "# print(smpl_data['2']['1']['0'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63a08268-e905-4013-ace0-1a0903a055bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose 72\n",
      "shape 10\n",
      "trans 1\n",
      "fitted_3d_pose 17\n"
     ]
    }
   ],
   "source": [
    "# for k, v in smpl_data['2']['1']['0'].items():\n",
    "#     print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73f670-e3e3-4d85-b46e-116447cfec2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
